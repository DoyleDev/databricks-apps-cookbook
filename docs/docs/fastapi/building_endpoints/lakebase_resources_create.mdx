---
sidebar_position: 3
---

# Create Lakebase Resources

This recipe demonstrates how to programmatically create Lakebase PostgreSQL resources in your Databricks workspace using FastAPI. This endpoint sets up a complete Lakebase environment including a database instance, catalog, and synced table pipeline.

:::warning Cost Alert
This endpoint creates billable resources in your Databricks environment including:
- A Lakebase PostgreSQL database instance
- A synced table pipeline

These resources will incur ongoing costs until deleted. Monitor your usage and delete resources when no longer needed using the delete endpoint.
:::

:::info What Gets Created
1. **Database Instance**: A managed PostgreSQL instance with configurable capacity and nodes
2. **Database Catalog**: A Unity Catalog that connects to the PostgreSQL database
3. **Synced Table**: A pipeline that syncs data from `samples.tpch.orders` to your PostgreSQL instance
4. **User Role**: Superuser role for the current user with full database permissions
:::

## Prerequisites

Before using this endpoint, ensure you have:
- Configured your `.env` file with Lakebase settings
- Appropriate Databricks workspace permissions to create database instances, pipelines, and catalogs
- Understanding of the cost implications

## Environment Variables

```bash title=".env"
LAKEBASE_INSTANCE_NAME=my-lakebase-instance
LAKEBASE_DATABASE_NAME=my_database  
LAKEBASE_CATALOG_NAME=my-pg-catalog
SYNCHED_TABLE_STORAGE_CATALOG=my_storage_catalog
SYNCHED_TABLE_STORAGE_SCHEMA=my_storage_schema
```

## Code Snippet

```python title="routes/v1/lakebase.py"
import logging
import os

from databricks.sdk import WorkspaceClient
from databricks.sdk.service.database import (
    DatabaseCatalog,
    DatabaseInstance,
    DatabaseInstanceRole,
    DatabaseInstanceRoleAttributes,
    DatabaseInstanceRoleIdentityType,
    DatabaseInstanceRoleMembershipRole,
    NewPipelineSpec,
    SyncedDatabaseTable,
    SyncedTableSchedulingPolicy,
    SyncedTableSpec,
)
from fastapi import APIRouter, HTTPException, Query

from models.lakebase import LakebaseResourcesResponse

router = APIRouter(tags=["lakebase"])
w = WorkspaceClient()

@router.post(
    "/resources/create-lakebase-resources",
    response_model=LakebaseResourcesResponse,
    summary="Create Lakebase Resources",
)
async def create_lakebase_resources(
    create_resources: bool = Query(
        description="""üö® This endpoint creates resources in your Databricks environment that will incur a cost. 
        By setting this value to true you understand the costs associated with this action. üö®
        ‚åõÔ∏è This endpoint may take a few minutes to complete.‚åõÔ∏è""",
    ),
    capacity: str = Query("CU_1", description="Capacity of the Lakebase instance"),
    node_count: int = Query(1, description="Number of nodes in the Lakebase instance"),
    enable_readable_secondaries: bool = Query(
        False, description="Enable readable secondaries"
    ),
    retention_window_in_days: int = Query(
        7, description="Retention window in days for the Lakebase instance"
    ),
):
    if not create_resources:
        return LakebaseResourcesResponse(
            instance="",
            catalog="",
            synced_table="",
            message="No resources were created (create_resources=False)",
        )
    
    instance_name = os.getenv("LAKEBASE_INSTANCE_NAME", f"{current_user_id}-lakebase-demo")
    
    # Create database instance
    instance = DatabaseInstance(
        name=instance_name,
        capacity=capacity,
        node_count=node_count,
        enable_readable_secondaries=enable_readable_secondaries,
        retention_window_in_days=retention_window_in_days,
    )
    
    instance_create = w.database.create_database_instance_and_wait(instance)
    
    # Create catalog and synced table
    # ... (full implementation in the repository)
    
    return LakebaseResourcesResponse(
        instance=instance_create.name,
        catalog=database_create.name,
        synced_table=pipeline_id,
        message=message,
    )
```

## Request Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `create_resources` | boolean | Required | **Must be `true` to create resources.** Confirms understanding of cost implications |
| `capacity` | string | `"CU_1"` | Compute capacity for the database instance (CU_1, CU_2, etc.) |
| `node_count` | integer | `1` | Number of nodes in the database instance |
| `enable_readable_secondaries` | boolean | `false` | Enable read-only secondary replicas for performance |
| `retention_window_in_days` | integer | `7` | Number of days to retain transaction logs |

## Response Format

```json
{
  "instance": "user123-lakebase-demo",
  "catalog": "user123-pg-catalog", 
  "synced_table": "pipeline-abc123",
  "message": "Resources created successfully. Synced table pipeline abc123 is provisioning asynchronously. Monitor progress at: https://workspace.databricks.com/pipelines/abc123"
}
```

## Usage Example

:::info 
For simplicity, run the fastapi locally and use the swagger ui to create resources.
:::

### Using curl

```bash
# Create Lakebase resources with default settings
curl -X POST "http://localhost:8000/api/v1/resources/create-lakebase-resources?create_resources=true"

# Create with custom capacity and multiple nodes
curl -X POST "http://localhost:8000/api/v1/resources/create-lakebase-resources?create_resources=true&capacity=CU_2&node_count=2&enable_readable_secondaries=true"
```

### Using Python requests

```python
import requests

response = requests.post(
    "http://localhost:8000/api/v1/resources/create-lakebase-resources",
    params={
        "create_resources": True,
        "capacity": "CU_2",
        "node_count": 2,
        "enable_readable_secondaries": True,
        "retention_window_in_days": 14
    }
)

result = response.json()
print(f"Instance created: {result['instance']}")
print(f"Pipeline URL: {result['message'].split('Monitor progress at: ')[-1]}")
```

## Error Handling

Common error scenarios and responses:

### Instance Already Exists
```json
{
  "instance": "existing-instance-name",
  "catalog": "",
  "synced_table": "",
  "message": "Instance already exists, skipping creation."
}
```

### Insufficient Permissions
```json
{
  "detail": "Error checking instance existence: User does not have permission to create database instances"
}
```

### Resource Creation Timeout
The endpoint uses `create_database_instance_and_wait()` which will wait for the instance to be fully provisioned. This can take several minutes.

## Best Practices

1. **Cost Management**: Always delete resources when done testing to avoid ongoing charges
2. **Environment Separation**: Use different instance names for dev/staging/prod environments
3. **Monitor Pipeline**: Check the pipeline URL in the response to monitor sync progress
4. **Error Handling**: Implement proper error handling for timeouts and permission issues
5. **Resource Naming**: Use descriptive instance names to avoid confusion

## Related Endpoints

- [Delete Lakebase Resources](./lakebase_resources_delete.mdx) - Clean up created resources
- [Lakebase Orders](./lakebase_orders.mdx) - Query and manage the synced orders data

## Next Steps

After creating resources:
1. Wait for the synced table pipeline to complete (check the provided URL)
2. Restart your FastAPI application to load the new database connection
3. Test the orders endpoints to verify data synchronization
4. Monitor costs in your Databricks workspace